{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_patent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "415uhIj6DA7G"
      },
      "source": [
        "https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMGCqb408mLS"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDa_rYJZ-WER",
        "outputId": "681d2963-112d-4b6d-8a32-d3f5c788b009"
      },
      "source": [
        "%%writefile utils.py\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from IPython.display import HTML\n",
        "\n",
        "from itertools import chain\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "\n",
        "RANDOM_STATE = 50\n",
        "TRAIN_FRACTION = 0.7\n",
        "\n",
        "\n",
        "def get_model(model_name):\n",
        "    \"\"\"Retrieve a Keras model and embeddings\"\"\"\n",
        "    model = load_model(f'../models/{model_name}.h5')\n",
        "    embeddings = model.get_layer(index = 0)\n",
        "    embeddings = embeddings.get_weights()[0]\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
        "    embeddings = np.nan_to_num(embeddings)\n",
        "    word_idx = []\n",
        "    with open(f'../data/training-rnn.json', 'rb') as f:\n",
        "        for l in f:\n",
        "            word_idx.append(json.loads(l))\n",
        "        \n",
        "    word_idx = word_idx[0]\n",
        "    word_idx['UNK'] = 0\n",
        "    idx_word = {index: word for word, index in word_idx.items()}\n",
        "    return model, embeddings, word_idx, idx_word\n",
        "\n",
        "def get_embeddings(model):\n",
        "    \"\"\"Retrieve the embeddings in a model\"\"\"\n",
        "    embeddings = model.get_layer(index = 0)\n",
        "    embeddings = embeddings.get_weights()[0]\n",
        "    embeddings = embeddings / np.linalg.norm(embeddings, axis = 1).reshape((-1, 1))\n",
        "    embeddings = np.nan_to_num(embeddings)\n",
        "    return embeddings\n",
        "    \n",
        "def find_closest(query, embedding_matrix, word_idx, idx_word, n = 10):\n",
        "    \"\"\"Find closest words to a query word in embeddings\"\"\"\n",
        "    \n",
        "    idx = word_idx.get(query, None)\n",
        "    # Handle case where query is not in vocab\n",
        "    if idx is None:\n",
        "        print(f'{query} not found in vocab.')\n",
        "        return\n",
        "    else:\n",
        "        vec = embedding_matrix[idx]\n",
        "        # Handle case where word doesn't have an embedding\n",
        "        if np.all(vec == 0):\n",
        "            print(f'{query} has no pre-trained embedding.')\n",
        "            return\n",
        "        else:\n",
        "            # Calculate distance between vector and all others\n",
        "            dists = np.dot(embedding_matrix, vec)\n",
        "            \n",
        "            # Sort indexes in reverse order\n",
        "            idxs = np.argsort(dists)[::-1][:n]\n",
        "            sorted_dists = dists[idxs]\n",
        "            closest = [idx_word[i] for i in idxs]\n",
        "            \n",
        "    print(f'Query: {query}\\n')\n",
        "    # Print out the word and cosine distances\n",
        "    for word, dist in zip(closest, sorted_dists):\n",
        "        print(f'Word: {word:15} Cosine Similarity: {round(dist, 4)}')\n",
        "        \n",
        "def format_sequence(s):\n",
        "    \"\"\"Add spaces around punctuation and remove references to images/citations.\"\"\"\n",
        "    \n",
        "    # Add spaces around punctuation\n",
        "    s =  re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', s)\n",
        "    \n",
        "    # Remove references to figures\n",
        "    s = re.sub(r'\\((\\d+)\\)', r'', s)\n",
        "    \n",
        "    # Remove double spaces\n",
        "    s = re.sub(r'\\s\\s', ' ', s)\n",
        "    return s\n",
        "\n",
        "def remove_spaces(s):\n",
        "    \"\"\"Remove spaces around punctuation\"\"\"\n",
        "    s = re.sub(r'\\s+([.,;?])', r'\\1', s)\n",
        "    \n",
        "    return s\n",
        "\n",
        "\n",
        "def get_data(file, filters='!\"%;[\\\\]^_`{|}~\\t\\n', training_len=50,\n",
        "             lower=False):\n",
        "    \"\"\"Retrieve formatted training and validation data from a file\"\"\"\n",
        "    \n",
        "    data = pd.read_csv(file, parse_dates=['patent_date']).dropna(subset = ['patent_abstract'])\n",
        "    abstracts = [format_sequence(a) for a in list(data['patent_abstract'])]\n",
        "    word_idx, idx_word, num_words, word_counts, texts, sequences, features, labels = make_sequences(\n",
        "        abstracts, training_len, lower, filters)\n",
        "    X_train, X_valid, y_train, y_valid = create_train_valid(features, labels, num_words)\n",
        "    training_dict = {'X_train': X_train, 'X_valid': X_valid, \n",
        "                     'y_train': y_train, 'y_valid': y_valid}\n",
        "    return training_dict, word_idx, idx_word, sequences\n",
        "\n",
        "def create_train_valid(features,\n",
        "                       labels,\n",
        "                       num_words,\n",
        "                       train_fraction=0.7):\n",
        "    \"\"\"Create training and validation features and labels.\"\"\"\n",
        "    \n",
        "    # Randomly shuffle features and labels\n",
        "    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n",
        "\n",
        "    # Decide on number of samples for training\n",
        "    train_end = int(train_fraction * len(labels))\n",
        "\n",
        "    train_features = np.array(features[:train_end])\n",
        "    valid_features = np.array(features[train_end:])\n",
        "\n",
        "    train_labels = labels[:train_end]\n",
        "    valid_labels = labels[train_end:]\n",
        "\n",
        "    # Convert to arrays\n",
        "    X_train, X_valid = np.array(train_features), np.array(valid_features)\n",
        "\n",
        "    # Using int8 for memory savings\n",
        "    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n",
        "    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n",
        "\n",
        "    # One hot encoding of labels\n",
        "    for example_index, word_index in enumerate(train_labels):\n",
        "        y_train[example_index, word_index] = 1\n",
        "\n",
        "    for example_index, word_index in enumerate(valid_labels):\n",
        "        y_valid[example_index, word_index] = 1\n",
        "\n",
        "    # Memory management\n",
        "    import gc\n",
        "    gc.enable()\n",
        "    del features, labels, train_features, valid_features, train_labels, valid_labels\n",
        "    gc.collect()\n",
        "\n",
        "    return X_train, X_valid, y_train, y_valid\n",
        "\n",
        "def make_sequences(texts, training_length = 50,\n",
        "                   lower = True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
        "    \n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    \n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "    \n",
        "    print(f'There are {num_words} unique words.')\n",
        "    \n",
        "    # Convert text to sequences of integers\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "    \n",
        "    # Limit to sequences with more than training length tokens\n",
        "    seq_lengths = [len(x) for x in sequences]\n",
        "    over_idx = [i for i, l in enumerate(seq_lengths) if l > (training_length + 20)]\n",
        "    \n",
        "    new_texts = []\n",
        "    new_sequences = []\n",
        "    \n",
        "    # Only keep sequences with more than training length tokens\n",
        "    for i in over_idx:\n",
        "        new_texts.append(texts[i])\n",
        "        new_sequences.append(sequences[i])\n",
        "        \n",
        "    features = []\n",
        "    labels = []\n",
        "    \n",
        "    # Iterate through the sequences of tokens\n",
        "    for seq in new_sequences:\n",
        "        \n",
        "        # Create multiple training examples from each sequence\n",
        "        for i in range(training_length, len(seq)):\n",
        "            # Extract the features and label\n",
        "            extract = seq[i - training_length: i + 1]\n",
        "            \n",
        "            # Set the features and label\n",
        "            features.append(extract[:-1])\n",
        "            labels.append(extract[-1])\n",
        "    \n",
        "    print(f'There are {len(features)} sequences.')\n",
        "    \n",
        "    # Return everything needed for setting up the model\n",
        "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels\n",
        "\n",
        "def generate_output(model,\n",
        "                    sequences,\n",
        "                    idx_word,\n",
        "                    seed_length=50,\n",
        "                    new_words=50,\n",
        "                    diversity=1,\n",
        "                    return_output=False,\n",
        "                    n_gen=1):\n",
        "    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n",
        "\n",
        "    # Choose a random sequence\n",
        "    seq = random.choice(sequences)\n",
        "\n",
        "    # Choose a random starting point\n",
        "    seed_idx = random.randint(0, len(seq) - seed_length - 10)\n",
        "    # Ending index for seed\n",
        "    end_idx = seed_idx + seed_length\n",
        "\n",
        "    gen_list = []\n",
        "\n",
        "    for n in range(n_gen):\n",
        "        # Extract the seed sequence\n",
        "        seed = seq[seed_idx:end_idx]\n",
        "        original_sequence = [idx_word[i] for i in seed]\n",
        "        generated = seed[:] + ['#']\n",
        "\n",
        "        # Find the actual entire sequence\n",
        "        actual = generated[:] + seq[end_idx:end_idx + new_words]\n",
        "\n",
        "        # Keep adding new words\n",
        "        for i in range(new_words):\n",
        "\n",
        "            # Make a prediction from the seed\n",
        "            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n",
        "                np.float64)\n",
        "\n",
        "            # Diversify\n",
        "            preds = np.log(preds) / diversity\n",
        "            exp_preds = np.exp(preds)\n",
        "\n",
        "            # Softmax\n",
        "            preds = exp_preds / sum(exp_preds)\n",
        "\n",
        "            # Choose the next word\n",
        "            probas = np.random.multinomial(1, preds, 1)[0]\n",
        "\n",
        "            next_idx = np.argmax(probas)\n",
        "\n",
        "            # New seed adds on old word\n",
        "            #             seed = seed[1:] + [next_idx]\n",
        "            seed += [next_idx]\n",
        "            generated.append(next_idx)\n",
        "\n",
        "        # Showing generated and actual abstract\n",
        "        n = []\n",
        "\n",
        "        for i in generated:\n",
        "            n.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "        gen_list.append(n)\n",
        "\n",
        "    a = []\n",
        "\n",
        "    for i in actual:\n",
        "        a.append(idx_word.get(i, '< --- >'))\n",
        "\n",
        "    a = a[seed_length:]\n",
        "\n",
        "    gen_list = [gen[seed_length:seed_length + len(a)] for gen in gen_list]\n",
        "\n",
        "    if return_output:\n",
        "        return original_sequence, gen_list, a\n",
        "\n",
        "    # HTML formatting\n",
        "    seed_html = ''\n",
        "    seed_html = addContent(seed_html, header(\n",
        "        'Seed Sequence', color='darkblue'))\n",
        "    seed_html = addContent(seed_html,\n",
        "                           box(remove_spaces(' '.join(original_sequence))))\n",
        "\n",
        "    gen_html = ''\n",
        "    gen_html = addContent(gen_html, header('RNN Generated', color='darkred'))\n",
        "    gen_html = addContent(gen_html, box(remove_spaces(' '.join(gen_list[0]))))\n",
        "\n",
        "    a_html = ''\n",
        "    a_html = addContent(a_html, header('Actual', color='darkgreen'))\n",
        "    a_html = addContent(a_html, box(remove_spaces(' '.join(a))))\n",
        "\n",
        "    return seed_html, gen_html, a_html\n",
        "\n",
        "\n",
        "\n",
        "def header(text, color = 'black', gen_text = None):\n",
        "    if gen_text:\n",
        "        raw_html = f'<h1 style=\"color: {color};\"><p><center>' + str(\n",
        "        text) + '<span style=\"color: red\">' + str(gen_text) + '</center></p></h1>'\n",
        "    else:\n",
        "        raw_html = f'<h1 style=\"color: {color};\"><center>' + str(\n",
        "            text) + '</center></h1>'\n",
        "    return raw_html\n",
        "\n",
        "\n",
        "def box(text, gen_text=None):\n",
        "    if gen_text:\n",
        "        raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>' + str(\n",
        "            text) +'<span style=\"color: red\">' + str(gen_text) + '</p></div>'\n",
        "\n",
        "    else:\n",
        "        raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\">' + str(\n",
        "            text) + '</div>'\n",
        "    return raw_html\n",
        "\n",
        "\n",
        "def addContent(old_html, raw_html):\n",
        "    old_html += raw_html\n",
        "    return old_html\n",
        "\n",
        "def seed_sequence(model, s, word_idx, idx_word, \n",
        "                  diversity = 0.75, num_words = 50):\n",
        "    \"\"\"Generate output starting from a seed sequence.\"\"\"\n",
        "    # Original formated text\n",
        "    start = format_sequence(s).split()\n",
        "    gen = []\n",
        "    s = start[:]\n",
        "    # Generate output\n",
        "    for _ in range(num_words):\n",
        "        # Conver to arry\n",
        "        x = np.array([word_idx.get(word, 0) for word in s]).reshape((1, -1))\n",
        "\n",
        "        # Make predictions\n",
        "        preds = model.predict(x)[0].astype(float)\n",
        "\n",
        "        # Diversify\n",
        "        preds = np.log(preds) / diversity\n",
        "        exp_preds = np.exp(preds)\n",
        "        # Softmax\n",
        "        preds = exp_preds / np.sum(exp_preds)\n",
        "        # Pick next index\n",
        "        next_idx = np.argmax(np.random.multinomial(1, preds, size = 1))\n",
        "        s.append(idx_word[next_idx])\n",
        "        gen.append(idx_word[next_idx])\n",
        "    \n",
        "    # Formatting in html\n",
        "    start = remove_spaces(' '.join(start)) + ' '\n",
        "    gen = remove_spaces(' '.join(gen)) \n",
        "    html = ''\n",
        "    html = addContent(html, header('Input Seed ', color = 'black', gen_text = 'Network Output'))\n",
        "    html = addContent(html, box(start, gen))\n",
        "    return html\n",
        "\n",
        "def guess_human(model, sequences, idx_word, seed_length=50):\n",
        "    \"\"\"Produce 2 RNN sequences and play game to compare to actaul.\n",
        "       Diversity is randomly set between 0.5 and 1.25\"\"\"\n",
        "    \n",
        "    new_words = np.random.randint(10, 50)\n",
        "    diversity = np.random.uniform(0.5, 1.25)\n",
        "    sequence, gen_list, actual = generate_output(model, sequences, idx_word, seed_length, new_words,\n",
        "                                                 diversity=diversity, return_output=True, n_gen = 2)\n",
        "    gen_0, gen_1 = gen_list\n",
        "    \n",
        "    output = {'sequence': remove_spaces(' '.join(sequence)),\n",
        "              'computer0': remove_spaces(' '.join(gen_0)),\n",
        "              'computer1': remove_spaces(' '.join(gen_1)),\n",
        "              'human': remove_spaces(' '.join(actual))}\n",
        "    \n",
        "    print(f\"Seed Sequence: {output['sequence']}\\n\")\n",
        "    \n",
        "    choices = ['human', 'computer0', 'computer1']\n",
        "          \n",
        "    selected = []\n",
        "    i = 0\n",
        "    while len(selected) < 3:\n",
        "        choice = random.choice(choices)\n",
        "        selected.append(choice)\n",
        "        print(f'\\nOption {i + 1} {output[choice]}')\n",
        "        choices.remove(selected[-1])\n",
        "        i += 1\n",
        "    \n",
        "    print('\\n')\n",
        "    guess = int(input('Enter option you think is human (1-3): ')) - 1\n",
        "    print('\\n')\n",
        "    \n",
        "    if guess == np.where(np.array(selected) == 'human')[0][0]:\n",
        "        print('*' * 3 + 'Correct' + '*' * 3 + '\\n')\n",
        "        print('-' * 60)\n",
        "        print('Ordering: ', selected)\n",
        "    else:\n",
        "        print('*' * 3 + 'Incorrect' + '*' * 3 + '\\n')\n",
        "        print('-' * 60)\n",
        "        print('Correct Ordering: ', selected)\n",
        "          \n",
        "    print('Diversity', round(diversity, 2))\n",
        "    \n",
        "def make_sequences_new(texts,\n",
        "                   training_length=50,\n",
        "                   lower=True,\n",
        "                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n",
        "    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n",
        "\n",
        "    # Create the tokenizer object and train on texts\n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "\n",
        "    # Convert text to sequences of integers\n",
        "    sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "    # Limit to sequences with more than (training length + 20) tokens\n",
        "    seq_lengths = [len(x) for x in sequences]\n",
        "    over_idx = [\n",
        "        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n",
        "    ]\n",
        "\n",
        "    new_texts = []\n",
        "\n",
        "    # Only keep sequences with more than training length tokens\n",
        "    for i in over_idx:\n",
        "        new_texts.append(texts[i])\n",
        "    \n",
        "    tokenizer = Tokenizer(lower=lower, filters=filters)\n",
        "    # Refit on long texts\n",
        "    tokenizer.fit_on_texts(new_texts)\n",
        "    new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "    \n",
        "    # Create look-up dictionaries and reverse look-ups\n",
        "    word_idx = tokenizer.word_index\n",
        "    idx_word = tokenizer.index_word\n",
        "    num_words = len(word_idx) + 1\n",
        "    word_counts = tokenizer.word_counts\n",
        "\n",
        "    print(f'There are {num_words} unique words.')\n",
        "\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    # Iterate through the sequences of tokens\n",
        "    for seq in new_sequences:\n",
        "\n",
        "        # Create multiple training examples from each sequence\n",
        "        for i in range(training_length, len(seq)):\n",
        "            # Extract the features and label\n",
        "            extract = seq[i - training_length:i + 1]\n",
        "\n",
        "            # Set the features and label\n",
        "            features.append(extract[:-1])\n",
        "            labels.append(extract[-1])\n",
        "\n",
        "    print(f'There are {len(features)} training sequences.')\n",
        "\n",
        "    # Return everything needed for setting up the model\n",
        "    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, features, labels"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5kldVzc8-5q"
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "from IPython.display import HTML\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category = RuntimeWarning)\n",
        "warnings.filterwarnings('ignore', category = UserWarning)\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from utils import get_data, generate_output, guess_human, seed_sequence, get_embeddings, find_closest"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "id": "YD7o-Da_8-8a",
        "outputId": "b1647fcc-18fd-4ae9-a0d3-290986da304f"
      },
      "source": [
        "data = pd.read_csv('/content/neural_network_patent_query.csv')\n",
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>patent_abstract</th>\n",
              "      <th>patent_date</th>\n",
              "      <th>patent_number</th>\n",
              "      <th>patent_title</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>\" A \"\"Barometer\"\" Neuron enhances stability in...</td>\n",
              "      <td>1996-07-09</td>\n",
              "      <td>5535303</td>\n",
              "      <td>\"\"\"Barometer\"\" neuron for a neural network\"</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\" This invention is a novel high-speed neural ...</td>\n",
              "      <td>1993-10-19</td>\n",
              "      <td>5255349</td>\n",
              "      <td>\"Electronic neural network for solving \"\"trave...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>An optical information processor for use as a ...</td>\n",
              "      <td>1995-01-17</td>\n",
              "      <td>5383042</td>\n",
              "      <td>3 layer liquid crystal neural network with out...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A method and system for intelligent control of...</td>\n",
              "      <td>2001-01-02</td>\n",
              "      <td>6169981</td>\n",
              "      <td>3-brain architecture for an intelligent decisi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>A method and system for intelligent control of...</td>\n",
              "      <td>2003-06-17</td>\n",
              "      <td>6581048</td>\n",
              "      <td>3-brain architecture for an intelligent decisi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     patent_abstract  ...                                       patent_title\n",
              "0  \" A \"\"Barometer\"\" Neuron enhances stability in...  ...        \"\"\"Barometer\"\" neuron for a neural network\"\n",
              "1  \" This invention is a novel high-speed neural ...  ...  \"Electronic neural network for solving \"\"trave...\n",
              "2  An optical information processor for use as a ...  ...  3 layer liquid crystal neural network with out...\n",
              "3  A method and system for intelligent control of...  ...  3-brain architecture for an intelligent decisi...\n",
              "4  A method and system for intelligent control of...  ...  3-brain architecture for an intelligent decisi...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMahheId8-_M",
        "outputId": "78362f6b-7c91-4c03-ba7d-5d66a108c6b2"
      },
      "source": [
        "training_dict, word_idx, idx_word, sequences = get_data('/content/neural_network_patent_query.csv', training_len = 50)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 16192 unique words.\n",
            "There are 318563 sequences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "503kKp2n8_B1",
        "outputId": "04263328-32ce-416a-8460-0d1ff63ff652"
      },
      "source": [
        "training_dict['X_train'][:2]\n",
        "training_dict['y_train'][:2]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  117,     7,   141,   277,     4,    18,    81,   110,    10,\n",
              "          219,    29,     1,   952,  2453,    19,     5,     6,     1,\n",
              "          117,    10,   182,  2166,    21,     1,    81,   178,     4,\n",
              "           13,   117,   894,    14,  6163,     7,   302,     1,     9,\n",
              "            8,    29,    33,    23,    74,   428,     7,   692,     1,\n",
              "           81,   183,     4,    13,   117],\n",
              "       [    6,    41,     2,    87,     3,  1340,    79,     7,     1,\n",
              "          409,   543,    22,   484,     6,     2,  2113,   728,    24,\n",
              "            1,   178,     3,     1,  1820,    55,    14, 13942,  7240,\n",
              "          244,     5,    14, 13943,  7240,   244,     5,     2,  2113,\n",
              "         7240,   244,     5,     2,    38,  9292,   244,     2,    49,\n",
              "         9292,   244,    14,    22, 13944]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0]], dtype=int8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdIMO7ye8_Ep",
        "outputId": "41975b11-8f8a-4a1e-f24a-66fcce9734d4"
      },
      "source": [
        "for i, sequence in enumerate(training_dict['X_train'][:2]):\n",
        "    text = []\n",
        "    for idx in sequence:\n",
        "        text.append(idx_word[idx])\n",
        "        \n",
        "    print('Features: ' + ' '.join(text) + '\\n')\n",
        "    print('Label: ' + idx_word[np.argmax(training_dict['y_train'][i])] + '\\n')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features: user to provide samples . A recognition operation is performed on the user's handwritten input , and the user is not satisfied with the recognition result . The user selects an option to train the neural network on one or more characters to improve the recognition results . The user\n",
            "\n",
            "Label: is\n",
            "\n",
            "Features: and includes a number of amplifiers corresponding to the N bit output sum and a carry generation from the result of the adding process an augend input-synapse group , an addend input-synapse group , a carry input-synapse group , a first bias-synapse group a second bias-synapse group an output feedback-synapse\n",
            "\n",
            "Label: group\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuOFw88g8_HT"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OSLgaqr8_J-",
        "outputId": "1b67c63e-4d89-4996-f43e-68beb4b9282d"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=len(word_idx) + 1, output_dim=100, weights=None, trainable=True)) # Embedding layer\n",
        "model.add(LSTM(64, return_sequences=False, dropout=0.1, recurrent_dropout=0.1))   # Recurrent layer\n",
        "model.add(Dense(64, activation='relu'))   # Fully connected layer\n",
        "model.add(Dropout(0.5))   # Dropout for regularization\n",
        "model.add(Dense(len(word_idx) + 1, activation='softmax'))   # Output layer\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, None, 100)         1619200   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 64)                42240     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 16192)             1052480   \n",
            "=================================================================\n",
            "Total params: 2,718,080\n",
            "Trainable params: 2,718,080\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFw6XQRg8_Mw",
        "outputId": "ba689eef-55db-473e-ccde-c45433ae7531"
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load in model and demonstrate training\n",
        "model = load_model('/content/train-embeddings-rnn.h5')\n",
        "h = model.fit(training_dict['X_train'], training_dict['y_train'], epochs = 5, batch_size = 2048, \n",
        "          validation_data = (training_dict['X_valid'], training_dict['y_valid']), \n",
        "          verbose = 1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "109/109 [==============================] - 347s 3s/step - loss: 3.7551 - accuracy: 0.2949 - val_loss: 5.1504 - val_accuracy: 0.2678\n",
            "Epoch 2/5\n",
            "109/109 [==============================] - 335s 3s/step - loss: 3.7405 - accuracy: 0.2958 - val_loss: 5.1550 - val_accuracy: 0.2682\n",
            "Epoch 3/5\n",
            "109/109 [==============================] - 341s 3s/step - loss: 3.7273 - accuracy: 0.2973 - val_loss: 5.1466 - val_accuracy: 0.2686\n",
            "Epoch 4/5\n",
            "109/109 [==============================] - 344s 3s/step - loss: 3.7123 - accuracy: 0.2985 - val_loss: 5.1637 - val_accuracy: 0.2686\n",
            "Epoch 5/5\n",
            "109/109 [==============================] - 341s 3s/step - loss: 3.7013 - accuracy: 0.3003 - val_loss: 5.1642 - val_accuracy: 0.2692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWy0FPG78_Pb",
        "outputId": "9583bd38-74d6-4d8a-97c3-e7c266dbde05"
      },
      "source": [
        "model = load_model('/content/train-embeddings-rnn.h5')\n",
        "print('Model Performance: Log Loss and Accuracy on training data')\n",
        "model.evaluate(training_dict['X_train'], training_dict['y_train'], batch_size = 2048)\n",
        "\n",
        "print('\\nModel Performance: Log Loss and Accuracy on validation data')\n",
        "model.evaluate(training_dict['X_valid'], training_dict['y_valid'], batch_size = 2048)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance: Log Loss and Accuracy on training data\n",
            "109/109 [==============================] - 94s 857ms/step - loss: 3.2897 - accuracy: 0.3384\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3.289726495742798, 0.3384440839290619]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model Performance: Log Loss and Accuracy on validation data\n",
            "47/47 [==============================] - 40s 855ms/step - loss: 5.1321 - accuracy: 0.2672\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.132135391235352, 0.2671891450881958]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m53xAkDA8_VB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 553
        },
        "outputId": "3f99b42b-4dfe-4a26-b722-926fc3dc9963"
      },
      "source": [
        "for i in generate_output(model, sequences, idx_word, seed_length = 50, new_words = 30, diversity = 0.75):\n",
        "    HTML(i)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">developed so as to generalize them as an element technique, and provide modeling of a basic unit of bottom-up approach using the neural network by adding new values to the existing techniques. A network learning device builds up a network of basic units in a network section,</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > and data associated with the training set as a previous neural network. The ANN can also be used to compare the fuzzifier first speech parameters. The network is</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > acquires an input from a sensor input section for evaluating it, changes a coupling weight coefficient by using a correlation operation so that the evaluation value satisfies a predetermined</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2UFDKnX8_X2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "outputId": "e043e00a-cd7a-4bb0-f1fc-a77c58e3f98e"
      },
      "source": [
        "for i in generate_output(model, sequences, idx_word, seed_length = 30, new_words = 30, diversity = 1.5):\n",
        "    HTML(i)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkblue;\"><center>Seed Sequence</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">non-linearity circuit controls the modulator in dependence on the detector output. There are parallel arrays (10, 11, 12) of such modulators, non-linearity circuits and detectors (M, T</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkred;\"><center>RNN Generated</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- > 2, to firing application is carried out during response to target ANN, sequentially available of instead based on comparison outputs and sensing components to herein disclosed, a powertrain</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: darkgreen;\"><center>Actual</center></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\">< --- >, D, 30, 33, 34). The modulator, non-linearity circuits and detectors have components formed in a common semiconductor substrate, for example by VLSI techniques with a</div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgIFoQPN8_ag",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "6d973dea-6401-4764-9531-a72659291a50"
      },
      "source": [
        "s = 'This patent provides a basis for using a recurrent neural network to '\n",
        "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: black;\"><p><center>Input Seed <span style=\"color: red\">Network Output</center></p></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>This patent provides a basis for using a recurrent neural network to <span style=\"color: red\">at least one neural network model. The processor is used to refine values to derive a pattern to control</p></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJVbecKH8_dW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "9ca35e46-4b08-47b0-e29f-497934ec83cc"
      },
      "source": [
        "s = 'The cell state is passed along from one time step to another allowing the '\n",
        "HTML(seed_sequence(model, s, word_idx, idx_word, diversity = 0.75, num_words = 20))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<h1 style=\"color: black;\"><p><center>Input Seed <span style=\"color: red\">Network Output</center></p></h1><div style=\"border:1px inset black;padding:1em;font-size: 20px;\"> <p>The cell state is passed along from one time step to another allowing the <span style=\"color: red\">adjustment of a K-dimensional weighting-coefficient. The key of the combined connections are then also provided for each given output</p></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3pfGQjz8_f_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35bd0ea9-65bf-401b-e5df-aa39d8180451"
      },
      "source": [
        "guess_human(model, sequences, idx_word)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed Sequence: analysis system automatically analyzes and counts fluorescence signals present in biopsy tissue marked using Fluorescence in situ Hybridization (FISH). The user of the system specifies classes of a class network and process steps of a process hierarchy. Then pixel values in image slices of biopsy tissue are acquired\n",
            "\n",
            "\n",
            "Option 1 < --- > to arrive an end of the exchanged. Thresholding can described in confinement and this method are also disclosed. More generally, the system may then be used with the\n",
            "\n",
            "Option 2 < --- > in three dimensions. A computer-implemented network structure is generated by linking pixel values to objects of a data network according to the class network and process hierarchy. Objects associated\n",
            "\n",
            "Option 3 < --- > to increasing the back-propagation training. The original speech can include resampling, operating system model, or cooperatively, supervised with the resulting speech data in like. direct information\n",
            "\n",
            "\n",
            "Enter option you think is human (1-3): 1\n",
            "\n",
            "\n",
            "***Incorrect***\n",
            "\n",
            "------------------------------------------------------------\n",
            "Correct Ordering:  ['computer0', 'human', 'computer1']\n",
            "Diversity 1.07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQswppjZ8_ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1c7890-f101-47a7-dcf2-e2bd9d5fb5ff"
      },
      "source": [
        "guess_human(model, sequences, idx_word)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed Sequence: decision threshold, input by a decision threshold block. Additionally, a validity model is also provided which represents the reliability or validity of the output as a function of the number of data points in a given data region during training of the system model. This predicts\n",
            "\n",
            "\n",
            "Option 1 < --- > the input input to the output of the first neural network. The control unit is also provided to the neural network, a neural network, and an output memory for effecting the output of the time or other of a degree of\n",
            "\n",
            "Option 2 < --- > the target value and the output of the input signal, the second layer is a future set of samples. The system is then provided with the neural network. The neural network can be used to predict an output signal. The\n",
            "\n",
            "Option 3 < --- > the confidence in the predicted output which is also input to the decision processor. The decision processor therefore bases its decision on the predicted confidence and the predicted uncertainty. Additionally, the uncertainty output by the data preprocess block can be utilized\n",
            "\n",
            "\n",
            "Enter option you think is human (1-3): 1\n",
            "\n",
            "\n",
            "***Incorrect***\n",
            "\n",
            "------------------------------------------------------------\n",
            "Correct Ordering:  ['computer0', 'computer1', 'human']\n",
            "Diversity 0.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUAmxok08_lt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ceb8c22c-fa15-4016-dbd4-9aa4d203ca57"
      },
      "source": [
        "guess_human(model, sequences, idx_word)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Seed Sequence: is generated. A plurality of light receiving regions of a photovoltaic material generate signals which are fed into amplifiers and summed. The gain of the amplifiers represent the synaptic weights. The output the summed amplified signals is then sent to a portion of a liquid crystal light\n",
            "\n",
            "\n",
            "Option 1 < --- > structure. The artificial neural network is constructed to produce a first current for the floating gate. A\n",
            "\n",
            "Option 2 < --- >, the second group and the air-fuel ratio, using a neural network having a neural network, and\n",
            "\n",
            "Option 3 < --- > valve where that portion of the liquid crystal light valve is used to produce a normalized light output.\n",
            "\n",
            "\n",
            "Enter option you think is human (1-3): 1\n",
            "\n",
            "\n",
            "***Incorrect***\n",
            "\n",
            "------------------------------------------------------------\n",
            "Correct Ordering:  ['computer0', 'computer1', 'human']\n",
            "Diversity 0.67\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQSbozpc8_oa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9da8164-8abc-42c1-ca68-8afeb3fbcd7f"
      },
      "source": [
        "embeddings = get_embeddings(model)\n",
        "embeddings.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16192, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNsaAbYj8_rH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd1d77d9-9aaa-4c52-e721-8dc3270a261c"
      },
      "source": [
        "find_closest('network', embeddings, word_idx, idx_word)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: network\n",
            "\n",
            "Word: network         Cosine Similarity: 1.0\n",
            "Word: channel         Cosine Similarity: 0.7754999995231628\n",
            "Word: networks        Cosine Similarity: 0.7745000123977661\n",
            "Word: system          Cosine Similarity: 0.7559999823570251\n",
            "Word: program         Cosine Similarity: 0.7541999816894531\n",
            "Word: cable           Cosine Similarity: 0.7419999837875366\n",
            "Word: now             Cosine Similarity: 0.7297999858856201\n",
            "Word: programming     Cosine Similarity: 0.7179999947547913\n",
            "Word: web             Cosine Similarity: 0.7138000130653381\n",
            "Word: line            Cosine Similarity: 0.6915000081062317\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93hJKZ_YCDpm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ac485d-9b57-4941-cb97-ed88cc7aed1b"
      },
      "source": [
        "find_closest('data', embeddings, word_idx, idx_word)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: data\n",
            "\n",
            "Word: data            Cosine Similarity: 1.0\n",
            "Word: information     Cosine Similarity: 0.8185999989509583\n",
            "Word: numbers         Cosine Similarity: 0.683899998664856\n",
            "Word: database        Cosine Similarity: 0.6776000261306763\n",
            "Word: account         Cosine Similarity: 0.6575999855995178\n",
            "Word: report          Cosine Similarity: 0.6575999855995178\n",
            "Word: signals         Cosine Similarity: 0.6399999856948853\n",
            "Word: system          Cosine Similarity: 0.6377000212669373\n",
            "Word: statistics      Cosine Similarity: 0.6371999979019165\n",
            "Word: web             Cosine Similarity: 0.6359000205993652\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmdN9XGrCDuM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvSEGg-KCD0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c80b28e-0156-4770-ad57-0e18c9122751"
      },
      "source": [
        "import requests \n",
        "\n",
        "response = requests.get('https://google.com/') \n",
        "print(response) "
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpblypvDCD4x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjiL7ZB6CD9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f9e8cfa-b9b4-4302-e0a6-8bf56c533405"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "max_features = 10000\n",
        "max_len = 500\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features) \n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 500)\n",
            "x_test shape: (25000, 500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYmOQvV3CEB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e42d6a8-2a17-46b5-ea8d-89b5b39e12db"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "model = Sequential()\n",
        "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.MaxPooling1D(5))\n",
        "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
        "model.add(layers.GlobalMaxPooling1D())\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()\n",
        "model.compile(optimizer=RMSprop(lr=1e-4),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 128)          1280000   \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 494, 32)           28704     \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 98, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 92, 32)            7200      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,315,937\n",
            "Trainable params: 1,315,937\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "157/157 [==============================] - 74s 468ms/step - loss: 0.7156 - acc: 0.5300 - val_loss: 0.6833 - val_acc: 0.5744\n",
            "Epoch 2/10\n",
            "157/157 [==============================] - 72s 461ms/step - loss: 0.6585 - acc: 0.6766 - val_loss: 0.6587 - val_acc: 0.6330\n",
            "Epoch 3/10\n",
            "157/157 [==============================] - 72s 459ms/step - loss: 0.6038 - acc: 0.7710 - val_loss: 0.5856 - val_acc: 0.7516\n",
            "Epoch 4/10\n",
            "157/157 [==============================] - 72s 456ms/step - loss: 0.4920 - acc: 0.8223 - val_loss: 0.4743 - val_acc: 0.7970\n",
            "Epoch 5/10\n",
            "157/157 [==============================] - 72s 459ms/step - loss: 0.3851 - acc: 0.8586 - val_loss: 0.4233 - val_acc: 0.8386\n",
            "Epoch 6/10\n",
            "157/157 [==============================] - 71s 453ms/step - loss: 0.3247 - acc: 0.8849 - val_loss: 0.4138 - val_acc: 0.8520\n",
            "Epoch 7/10\n",
            "157/157 [==============================] - 72s 459ms/step - loss: 0.2839 - acc: 0.9002 - val_loss: 0.4282 - val_acc: 0.8588\n",
            "Epoch 8/10\n",
            "157/157 [==============================] - 72s 462ms/step - loss: 0.2523 - acc: 0.9149 - val_loss: 0.4259 - val_acc: 0.8674\n",
            "Epoch 9/10\n",
            "157/157 [==============================] - 73s 465ms/step - loss: 0.2308 - acc: 0.9244 - val_loss: 0.4351 - val_acc: 0.8704\n",
            "Epoch 10/10\n",
            "157/157 [==============================] - 73s 467ms/step - loss: 0.2072 - acc: 0.9330 - val_loss: 0.4425 - val_acc: 0.8744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWQE7QskCEGd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7MId5LiCEMC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw_sjo5kCEP2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IAWDepA8_t3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}